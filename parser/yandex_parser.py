import asyncio
import random
import re
import time
from datetime import datetime
from typing import List, Dict, Set
from bs4 import BeautifulSoup
import nodriver


class YandexPyroParser:
    """–ü–∞—Ä—Å–µ—Ä –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç –¥–ª—è –º–∞–≥–∞–∑–∏–Ω–æ–≤ –ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∏ –≤ –†–æ—Å—Ç–æ–≤–µ-–Ω–∞-–î–æ–Ω—É"""

    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser = None
        self.all_urls: Set[str] = set()
        self.results: List[Dict] = []

        # –û–±–ª–∞—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ (—Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –≥–æ—Ä–æ–¥–∞)
        self.search_areas = [
            {
                "name": "–í–µ—Å—å –≥–æ—Ä–æ–¥ (–æ–±—â–∏–π –ø–æ–∏—Å–∫)",
                "url": "https://yandex.ru/maps/39/rostov-na-donu/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞/?ll=39.720451%2C47.232724&sll=39.720451%2C47.232724&sspn=0.672226%2C0.318267&z=11"
            },
            {
                "name": "–¶–µ–Ω—Ç—Ä –≥–æ—Ä–æ–¥–∞ –¥–µ—Ç–∞–ª—å–Ω–æ",
                "url": "https://yandex.ru/maps/39/rostov-na-donu/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞/?ll=39.720451%2C47.232724&sll=39.720451%2C47.232724&sspn=0.336113%2C0.159133&z=12"
            },
            {
                "name": "–°–µ–≤–µ—Ä–Ω—ã–µ —Ä–∞–π–æ–Ω—ã",
                "url": "https://yandex.ru/maps/39/rostov-na-donu/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞/?ll=39.720451%2C47.282724&sll=39.720451%2C47.282724&sspn=0.336113%2C0.159133&z=12"
            },
            {
                "name": "–Æ–∂–Ω—ã–µ —Ä–∞–π–æ–Ω—ã",
                "url": "https://yandex.ru/maps/39/rostov-na-donu/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞/?ll=39.720451%2C47.182724&sll=39.720451%2C47.182724&sspn=0.336113%2C0.159133&z=12"
            },
            {
                "name": "–ó–∞–ø–∞–¥–Ω—ã–µ —Ä–∞–π–æ–Ω—ã",
                "url": "https://yandex.ru/maps/39/rostov-na-donu/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞/?ll=39.620451%2C47.232724&sll=39.620451%2C47.232724&sspn=0.336113%2C0.159133&z=12"
            },
            {
                "name": "–í–æ—Å—Ç–æ—á–Ω—ã–µ —Ä–∞–π–æ–Ω—ã",
                "url": "https://yandex.ru/maps/39/rostov-na-donu/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞/?ll=39.820451%2C47.232724&sll=39.820451%2C47.232724&sspn=0.336113%2C0.159133&z=12"
            }
        ]

    async def init_browser(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            print("üöÄ –ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞...")
            self.browser = await nodriver.start(
                headless=self.headless,
                window_size=(1300, 900),
                disable_webgl=True,
                disable_extensions=True
            )
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            return False

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            if self.browser:
                await self.browser.stop()
                self.browser = None
        except Exception as e:
            print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")

    async def parse(self) -> List[Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("=" * 80)
        print("üî• –ü–ê–†–°–ï–† –ú–ê–ì–ê–ó–ò–ù–û–í –ü–ò–†–û–¢–ï–•–ù–ò–ö–ò - –†–û–°–¢–û–í-–ù–ê-–î–û–ù–£")
        print("=" * 80)

        self.start_time = time.time()
        self.results = []
        self.all_urls.clear()

        if not await self.init_browser():
            return []

        try:
            # 1. –ü–∞—Ä—Å–∏–º –≤—Å–µ –æ–±–ª–∞—Å—Ç–∏ –≥–æ—Ä–æ–¥–∞
            print(f"\nüéØ –ù–ê–ß–ò–ù–ê–ï–ú –ü–ê–†–°–ò–ù–ì {len(self.search_areas)} –û–ë–õ–ê–°–¢–ï–ô...")

            for i, area in enumerate(self.search_areas, 1):
                print(f"\n{'=' * 60}")
                print(f"–û–±–ª–∞—Å—Ç—å {i}/{len(self.search_areas)}: {area['name']}")
                print(f"{'=' * 60}")

                urls_before = len(self.all_urls)

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞ –¥–ª—è —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏
                print(f"üåê –û—Ç–∫—Ä—ã–≤–∞–µ–º: {area['name']}")
                page = await self.browser.get(area['url'])
                await asyncio.sleep(4)

                # –°–∫—Ä–∞–ø–∏–º —ç—Ç—É –æ–±–ª–∞—Å—Ç—å
                await self.smart_area_scroll(page)

                # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
                await self.collect_store_links(page)

                new_urls = len(self.all_urls) - urls_before
                print(f"‚úÖ –í –æ–±–ª–∞—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω–æ –º–∞–≥–∞–∑–∏–Ω–æ–≤: {new_urls}")

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±–ª–∞—Å—Ç—è–º–∏
                if i < len(self.search_areas):
                    await asyncio.sleep(random.uniform(5, 8))

            if not self.all_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏")
                return []

            print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –º–∞–≥–∞–∑–∏–Ω—ã: {len(self.all_urls)}")

            # 2. –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –º–∞–≥–∞–∑–∏–Ω
            print("\nüè™ –ü–ê–†–°–ò–ú –î–ê–ù–ù–´–ï –ú–ê–ì–ê–ó–ò–ù–û–í...")
            urls_list = list(self.all_urls)

            for i, url in enumerate(urls_list, 1):
                print(f"   {i}/{len(urls_list)}: {url}")
                data = await self.parse_store_page(url)
                if data:
                    self.results.append(data)
                    print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
                else:
                    print(f"      ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(urls_list):
                    await asyncio.sleep(random.uniform(3, 5))

            # 3. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            self.remove_duplicates()

            # 4. –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.print_statistics()

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    async def smart_area_scroll(self, page):
        """–°–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        max_scrolls = 30
        no_new_count = 0
        previous_count = 0

        for scroll_num in range(1, max_scrolls + 1):
            print(f"   üìç –°–∫—Ä–æ–ª–ª {scroll_num}/{max_scrolls}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            current_count_before = len(self.all_urls)

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫—Ä–æ–ª–ª
            await self.execute_scroll_strategies(page)
            await asyncio.sleep(random.uniform(1.5, 2.5))

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
            await self.collect_store_links(page)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—è–≤–∏–ª–∏—Å—å –ª–∏ –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
            new_urls = len(self.all_urls) - current_count_before

            if new_urls > 0:
                print(f"   üì• –ù–æ–≤—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤: {new_urls}")
                no_new_count = 0
            else:
                no_new_count += 1
                print(f"   üì≠ –ù–æ–≤—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ –Ω–µ—Ç ({no_new_count}/3)")

                if no_new_count >= 3:
                    print("   üèÅ –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏")
                    break

            # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è 3 —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ - –≤—ã—Ö–æ–¥–∏–º
            if len(self.all_urls) == previous_count:
                no_new_count += 1
            else:
                no_new_count = 0

            previous_count = len(self.all_urls)

            # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞
            await asyncio.sleep(random.uniform(0.5, 1))

    async def execute_scroll_strategies(self, page):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∫—Ä–æ–ª–ª–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
        try:
            # –°–∫—Ä–æ–ª–ª –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            await page.evaluate("""
                (function() {
                    // –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                    const containers = [
                        '.scroll__container',
                        '.scroll__container_width_narrow',
                        '.search-list-view__list-container',
                        '.sidebar-view__panel',
                        '.scrollable-container'
                    ];

                    for (const selector of containers) {
                        const container = document.querySelector(selector);
                        if (container && container.scrollHeight > container.clientHeight) {
                            container.scrollTop = container.scrollHeight;
                            console.log('–°–∫—Ä–æ–ª–ª –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: ' + selector);
                            return true;
                        }
                    }
                    return false;
                })();
            """)

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ —Å–∫—Ä–æ–ª–ª–∞: {e}")

    async def collect_store_links(self, page):
        """–°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ –º–∞–≥–∞–∑–∏–Ω—ã –∏–∑ —Å–ø–∏—Å–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            html = await page.get_content()
            soup = BeautifulSoup(html, 'html.parser')

            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã li —Å –∫–ª–∞—Å—Å–æ–º search-snippet-view
            store_items = soup.find_all('li', class_='search-snippet-view')

            urls_before = len(self.all_urls)

            for item in store_items:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ data-nosnippet
                nosnippet = item.find('span', attrs={'data-nosnippet': True})
                if nosnippet:
                    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ nosnippet
                    links = nosnippet.find_all('a', href=True)
                    for link in links:
                        href = link['href']
                        if self.is_store_url(href):
                            full_url = self.normalize_url(href)
                            if full_url:
                                self.all_urls.add(full_url)
                                break  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç data-nosnippet, –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ
                    links = item.find_all('a', href=True)
                    for link in links:
                        href = link['href']
                        if self.is_store_url(href):
                            full_url = self.normalize_url(href)
                            if full_url:
                                self.all_urls.add(full_url)
                                break

            new_urls = len(self.all_urls) - urls_before
            if new_urls > 0:
                print(f"   üì• –ù–∞–π–¥–µ–Ω–æ {new_urls} –Ω–æ–≤—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫: {e}")

    def is_store_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Å—ã–ª–∫–æ–π –Ω–∞ –º–∞–≥–∞–∑–∏–Ω"""
        if not url:
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        store_patterns = ['/org/', '/firm/', 'businessId=']

        for pattern in store_patterns:
            if pattern in url:
                return True

        return False

    def normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –º–∞–≥–∞–∑–∏–Ω"""
        if not url:
            return ""

        # –°–ø–∏—Å–æ–∫ –≤–∫–ª–∞–¥–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å
        tabs_to_remove = ['/reviews', '/photos', '/gallery', '/menu']

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–º–µ–Ω –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if url.startswith('//'):
            url = f"https:{url}"
        elif url.startswith('/'):
            url = f"https://yandex.ru{url}"
        elif not url.startswith('http'):
            return ""

        # –£–¥–∞–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
        url = url.split('?')[0].split('#')[0].strip()

        # –û–±—Ä–µ–∑–∞–µ–º –≤–∫–ª–∞–¥–∫–∏ (reviews, photos, gallery, menu)
        for tab in tabs_to_remove:
            if tab in url:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –≤–∫–ª–∞–¥–∫–∏ –∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω–µ—ë
                tab_index = url.find(tab)
                if tab_index != -1:
                    url = url[:tab_index]

        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω–µ—á–Ω—ã–µ —Å–ª–µ—à–∏
        url = url.rstrip('/')

        return url

    async def parse_store_page(self, url: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∞–≥–∞–∑–∏–Ω–∞"""
        try:
            print(f"      üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–∞–≥–∞–∑–∏–Ω–∞...")
            page = await self.browser.get(url)
            await asyncio.sleep(random.uniform(3, 4))

            # –ü–æ–ª—É—á–∞–µ–º HTML
            html = await page.get_content()

            # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
            data = self.parse_store_data(url, html)

            return data if data else None

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None

    def parse_store_data(self, url: str, html: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –º–∞–≥–∞–∑–∏–Ω–µ –∏–∑ HTML"""
        soup = BeautifulSoup(html, 'html.parser')

        data = {
            '–°—Å—ã–ª–∫–∞': url,
            '–ì–æ—Ä–æ–¥': '–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É',
            '–î–∞—Ç–∞ —Å–±–æ—Ä–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # 1. –ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞
        title_selectors = [
            'h1.orgpage-header-view__header',
            'h1.business-title-view__title',
            'h1.card-title-view__title',
            'h1[itemprop="name"]',
            '.orgpage-header-view__header',
            '.business-title-view__title',
            '.card-title-view__title'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text and len(text) > 2:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞'] = text
                    break

        # 2. –ê–¥—Ä–µ—Å
        address_selectors = [
            '[itemprop="address"]',
            '.business-contacts-view__address',
            '.card-address-view__address',
            '.orgpage-address-view__address-text',
            '.business-address-view__address',
            'address',
            '.location__description'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 5:
                    data['–ê–¥—Ä–µ—Å'] = text
                    break

        # –ü–†–û–í–ï–†–ö–ê: –Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–∞–≥–∞–∑–∏–Ω –∏–∑ –†–æ—Å—Ç–æ–≤–∞-–Ω–∞-–î–æ–Ω—É
        address = data.get('–ê–¥—Ä–µ—Å', '')
        if address:
            # –ü—Ä–∏–≤–æ–¥–∏–º –∞–¥—Ä–µ—Å –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            address_lower = address.lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –†–æ—Å—Ç–æ–≤–∞-–Ω–∞-–î–æ–Ω—É
            rostov_patterns = [
                '—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É',
                '—Ä–æ—Å—Ç–æ–≤ –Ω–∞ –¥–æ–Ω—É',
                '—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É,',
                '–≥.—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É',
                '–≥. —Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É',
                '–≥. —Ä–æ—Å—Ç–æ–≤',
                '–≥.—Ä–æ—Å—Ç–æ–≤',
                '—Ä–æ—Å—Ç–æ–≤,'
            ]

            # –§–ª–∞–≥, —á—Ç–æ –º–∞–≥–∞–∑–∏–Ω –∏–∑ –†–æ—Å—Ç–æ–≤–∞
            is_rostov = False

            for pattern in rostov_patterns:
                if pattern in address_lower:
                    is_rostov = True
                    break

            # –ï—Å–ª–∏ –º–∞–≥–∞–∑–∏–Ω –Ω–µ –∏–∑ –†–æ—Å—Ç–æ–≤–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
            if not is_rostov:
                print(f"      üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–≥–∞–∑–∏–Ω (–Ω–µ –∏–∑ –†–æ—Å—Ç–æ–≤–∞-–Ω–∞-–î–æ–Ω—É): {address}")
                return None
        else:
            # –ï—Å–ª–∏ –∞–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–∞–º –Ω—É–∂–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ—Ä–æ–¥—É - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            print(f"      ‚ö† –ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–≥–∞–∑–∏–Ω")
            return None

        # 3. –¢–µ–ª–µ—Ñ–æ–Ω - –∏—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É orgpage-phones-view__phone-number
        phones = []

        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É
        phone_elements = soup.find_all(class_='orgpage-phones-view__phone-number')
        for elem in phone_elements:
            phone_text = elem.get_text(strip=True)
            if phone_text:
                # –û—á–∏—â–∞–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞
                clean_phone = re.sub(r'[^\d\+]', '', phone_text)
                if clean_phone and len(clean_phone) >= 10 and clean_phone not in phones:
                    phones.append(clean_phone)

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        if not phones:
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Å tel:
            for link in soup.find_all('a', href=lambda x: x and x.startswith('tel:')):
                phone = link['href'].replace('tel:', '').strip()
                if phone:
                    clean_phone = re.sub(r'[^\d\+]', '', phone)
                    if clean_phone and clean_phone not in phones:
                        phones.append(clean_phone)

            # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
            text = soup.get_text()
            phone_patterns = [
                r'8\s?[\(\-]?\d{3}[\)\-]?\s?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}',
                r'\+7\s?[\(\-]?\d{3}[\)\-]?\s?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}',
                r'\(\d{3,4}\)\s?\d{2,3}[\s\-]\d{2}[\s\-]\d{2}'
            ]

            for pattern in phone_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    clean_phone = re.sub(r'[^\d\+]', '', match)
                    if clean_phone and len(clean_phone) >= 10 and clean_phone not in phones:
                        phones.append(clean_phone)

        if phones:
            data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones[:3])  # –ë–µ—Ä–µ–º –Ω–µ –±–æ–ª–µ–µ 3 –Ω–æ–º–µ—Ä–æ–≤

        # 4. –°–∞–π—Ç - –∏—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É business-urls-view__text
        site_found = False

        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É
        site_elements = soup.find_all(class_='business-urls-view__text')
        for elem in site_elements:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ href —É —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–ª–∏ —É —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ <a>
            if elem.name == 'a' and elem.get('href'):
                href = elem['href']
            else:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                link = elem.find('a')
                if link and link.get('href'):
                    href = link['href']
                else:
                    continue

            if href:
                clean_url = self.clean_website_url(href)
                if clean_url and not self.is_yandex_url(clean_url):
                    data['–°–∞–π—Ç'] = clean_url
                    site_found = True
                    break

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å–∞–π—Ç–∞
        if not site_found:
            site_selectors = [
                '.business-urls-view__link',
                '.card-website-view__link',
                '.orgpage-url-view__url',
                '.website-link'
            ]

            for selector in site_selectors:
                elem = soup.select_one(selector)
                if elem and elem.get('href'):
                    href = elem['href']
                    clean_url = self.clean_website_url(href)
                    if clean_url and not self.is_yandex_url(clean_url):
                        data['–°–∞–π—Ç'] = clean_url
                        break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–±—Ä–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è, –Ω–æ –µ—Å—Ç—å –∞–¥—Ä–µ—Å - –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∞—Å—Ç—å –∞–¥—Ä–µ—Å–∞ –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ
        if not data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞') and data.get('–ê–¥—Ä–µ—Å'):
            address_parts = data['–ê–¥—Ä–µ—Å'].split(',')
            if address_parts:
                data['–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞'] = address_parts[0].strip()

        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –Ω–µ—Ç –∞–¥—Ä–µ—Å–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if not data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞') and not data.get('–ê–¥—Ä–µ—Å'):
            return None

        return data

    def is_yandex_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Å—ã–ª–∫–æ–π –Ω–∞ –Ø–Ω–¥–µ–∫—Å"""
        if not url:
            return False

        yandex_domains = ['yandex.ru', 'yandex.com', 'ya.ru', 'yandex.net']
        url_lower = url.lower()

        for domain in yandex_domains:
            if domain in url_lower:
                return True

        return False

    def clean_website_url(self, url: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ URL —Å–∞–π—Ç–∞"""
        if not url:
            return ""

        # –û—á–∏—â–∞–µ–º URL
        url = url.split('?')[0].split('#')[0].strip()

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if url.startswith('//'):
            url = f"https:{url}"
        elif not url.startswith('http'):
            url = f"https://{url}"

        return url

    def remove_duplicates(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if not self.results:
            return

        unique_results = []
        seen = set()

        for item in self.results:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏
            url = item.get('–°—Å—ã–ª–∫–∞', '').lower().strip()

            if url:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∏–∑ URL
                # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ ID: /org/–Ω–∞–∑–≤–∞–Ω–∏–µ/ID/
                match = re.search(r'/(\d+)(?:/|$)', url)
                if match:
                    unique_id = match.group(1)
                    if unique_id not in seen:
                        seen.add(unique_id)
                        unique_results.append(item)
                elif url not in seen:
                    seen.add(url)
                    unique_results.append(item)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç URL, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–¥—Ä–µ—Å
                name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞', '').lower().strip()
                address = item.get('–ê–¥—Ä–µ—Å', '').lower().strip()

                if name and address:
                    key = f"{name}|{address}"
                    if key not in seen:
                        seen.add(key)
                        unique_results.append(item)
                elif name:
                    if name not in seen:
                        seen.add(name)
                        unique_results.append(item)
                elif address:
                    if address not in seen:
                        seen.add(address)
                        unique_results.append(item)
                else:
                    unique_results.append(item)

        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"üóë –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed}")

        self.results = unique_results

    def print_statistics(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "=" * 80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê")
        print("=" * 80)

        elapsed_time = time.time() - self.start_time
        minutes = int(elapsed_time // 60)
        seconds = int(elapsed_time % 60)

        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –º–∞–≥–∞–∑–∏–Ω–æ–≤: {len(self.results)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
        phones_count = sum(1 for r in self.results if r.get('–¢–µ–ª–µ—Ñ–æ–Ω'))
        sites_count = sum(1 for r in self.results if r.get('–°–∞–π—Ç'))

        print(f"üìû –ú–∞–≥–∞–∑–∏–Ω–æ–≤ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º: {phones_count}")
        print(f"üåê –ú–∞–≥–∞–∑–∏–Ω–æ–≤ —Å —Å–∞–π—Ç–æ–º: {sites_count}")
