import asyncio
import random
import re
import time
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from urllib.parse import urlparse

from bs4 import BeautifulSoup
import nodriver


class TwoGisPyroParser:
    """–ü–∞—Ä—Å–µ—Ä 2–ì–ò–° –¥–ª—è –º–∞–≥–∞–∑–∏–Ω–æ–≤ –ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∏ –≤ –†–æ—Å—Ç–æ–≤–µ-–Ω–∞-–î–æ–Ω—É."""

    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    BROWSER_CONFIG = {
        "headless": True,
        "window_size": (1300, 900),
        "disable_webgl": True,
        "disable_extensions": True
    }

    # SVG –ø—É—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    SVG_PATHS = {
        "address": "M5 11v2a6.82 6.82 0 0 1 4.17 1.41C10.75 15.62 11.53 18 11.5 22h1c0-4 .75-6.38 2.33-7.59A6.82 6.82 0 0 1 19 13v-2a7 7 0 0 0-7-7 7 7 0 0 0-7 7z",
        "phone": "M14 14l-1.08 1.45a13.61 13.61 0 0 1-4.37-4.37L10 10a18.47 18.47 0 0 0-.95-5.85L9 4H5.06a1 1 0 0 0-1 1.09 16 16 0 0 0 14.85 14.85 1 1 0 0 0 1.09-1V15h-.15A18.47 18.47 0 0 0 14 14z",
        "website": "M12 4a8 8 0 1 0 8 8 8 8 0 0 0-8-8zm5 9h-6l1-7h1v5.25l4 .75z"
    }

    # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    SELECTORS = {
        "search_results": [
            '.searchResults',
            '.listContainer',
            '.searchResults__list',
            '.searchResults__container',
            '[data-qa="search-results"]',
            '.searchTab__content'
        ],
        "scroll_containers": [
            '.searchResults__list',
            '.listContainer',
            '.searchResults__container',
            '.scroll__container',
            '[data-scroll]'
        ]
    }

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    PATTERNS = {
        "phone": [
            r'\+7\s?\(?\d{3}\)?\s?\d{3}[\s-]?\d{2}[\s-]?\d{2}',
            r'8\s?\(?\d{3}\)?\s?\d{3}[\s-]?\d{2}[\s-]?\d{2}',
            r'\(\d{3}\)\s?\d{3}[\s-]?\d{2}[\s-]?\d{2}'
        ],
        "domain": [
            r'^[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}$',
            r'^[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}\.[a-zA-Z]{2,}$',
            r'^www\.[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}$',
            r'^https?://[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}',
            r'^[–∞-—è–ê-–Ø—ë–Å0-9][–∞-—è–ê-–Ø—ë–Å0-9-]{0,61}[–∞-—è–ê-–Ø—ë–Å0-9]\.(ru|—Ä—Ñ|su|com|net|org)'
        ],
        "address": [
            r'–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É[,\s]+[–ê-–Ø–∞-—è—ë–Å0-9\s\-\.]+—É–ª\.[\s]*[–ê-–Ø–∞-—è—ë–Å\-]+[\s]*[,\s]*–¥\.[\s]*\d+',
            r'–≥\.\s*–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É[,\s]+[–ê-–Ø–∞-—è—ë–Å0-9\s\-\.]+',
            r'—É–ª\.\s*[–ê-–Ø–∞-—è—ë–Å\-]+[\s]*[,\s]*–¥\.\s*\d+[\s]*[,\s]*–≥\.\s*–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É'
        ]
    }

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser = None
        self.all_urls: Set[str] = set()
        self.results: List[Dict] = []
        self.start_time = None
        self.processed_ids: Set[str] = set()

        self.search_areas = [
            {
                "name": "–í–µ—Å—å –≥–æ—Ä–æ–¥ (–æ–±—â–∏–π –ø–æ–∏—Å–∫)",
                "url": "https://2gis.ru/rostov-on-don/search/–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞"
            }
        ]

    @property
    def source_name(self) -> str:
        return "2gis"

    async def init_browser(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            print("üöÄ –ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ 2GIS...")
            config = self.BROWSER_CONFIG.copy()
            config["headless"] = self.headless
            self.browser = await nodriver.start(**config)
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            return False

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            if self.browser:
                await self.browser.stop()
                self.browser = None
        except Exception as e:
            print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")

    async def parse(self) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        self._print_header()

        self.start_time = time.time()
        self.results = []
        self.all_urls.clear()
        self.processed_ids.clear()

        if not await self.init_browser():
            return []

        try:
            await self._parse_search_areas()

            if not self.all_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞–≥–∞–∑–∏–Ω—ã")
                return []

            urls_list = list(self.all_urls)
            await self._parse_store_pages(urls_list)

            self._remove_duplicates()
            self._print_final_stats()

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    # ========== –ü–ê–†–°–ò–ù–ì –û–ë–õ–ê–°–¢–ï–ô –ü–û–ò–°–ö–ê ==========

    async def _parse_search_areas(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ–±–ª–∞—Å—Ç–µ–π –ø–æ–∏—Å–∫–∞"""
        print(f"\nüéØ –ù–ê–ß–ò–ù–ê–ï–ú –ü–ê–†–°–ò–ù–ì {len(self.search_areas)} –û–ë–õ–ê–°–¢–ï–ô")
        print("-" * 50)

        for i, area in enumerate(self.search_areas, 1):
            urls_before = len(self.all_urls)

            print(f"\nüìç –û–±–ª–∞—Å—Ç—å {i}/{len(self.search_areas)}: {area['name']}")
            print(f"   URL: {area['url']}")

            await self._collect_urls_from_area(area['url'], area['name'])

            new_urls = len(self.all_urls) - urls_before
            print(f"‚úÖ –í –æ–±–ª–∞—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω–æ –º–∞–≥–∞–∑–∏–Ω–æ–≤: {new_urls}")
            print(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(self.all_urls)}")

            if i < len(self.search_areas):
                await asyncio.sleep(random.uniform(3, 5))

    async def _collect_urls_from_area(self, area_url: str, area_name: str) -> bool:
        """–°–±–æ—Ä URL –º–∞–≥–∞–∑–∏–Ω–æ–≤ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        try:
            print(f"   üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –≤ –æ–±–ª–∞—Å—Ç–∏: {area_name}")

            tab = await self.browser.get(area_url)
            await asyncio.sleep(random.uniform(4, 6))

            await self._click_search_results_if_needed(tab)

            print("   üì• –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
            initial_urls = await self._get_urls_from_current_page(tab)
            if initial_urls:
                self.all_urls.update(initial_urls)
                print(f"   üìä –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {len(initial_urls)} URL")

            print("   üìú –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
            await self._scroll_page_to_bottom(tab)

            current_urls = await self._get_urls_from_current_page(tab)
            if current_urls:
                previous_count = len(self.all_urls)
                self.all_urls.update(current_urls)
                new_urls = len(self.all_urls) - previous_count
                print(f"   üìé –í—Å–µ–≥–æ URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏: {len(self.all_urls)} (+{new_urls} –Ω–æ–≤—ã—Ö)")

            print("   üîç –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏...")
            await self._try_find_pagination_after_scroll(tab, current_page=1)

            print(f"   ‚úÖ –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ {area_name} –∑–∞–≤–µ—Ä—à–µ–Ω")
            return True

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ {area_name}: {str(e)[:100]}")
            return False

    async def _click_search_results_if_needed(self, tab):
        """–ö–ª–∏–∫–∞–µ—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å"""
        try:
            await asyncio.sleep(2)

            for selector in self.SELECTORS["search_results"]:
                element = await tab.query_selector(selector)
                if element:
                    print("   üñ± –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∫–ª–∏–∫–∞–µ–º...")
                    await element.click()
                    await asyncio.sleep(2)
                    break

            first_card = await tab.query_selector('.minicard')
            if first_card:
                await first_card.click()
                await asyncio.sleep(1)

        except Exception as e:
            print(f"   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∫–ª–∏–∫–Ω—É—Ç—å –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º: {str(e)[:50]}")

    async def _scroll_page_to_bottom(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –í–°–ï —Å–∫—Ä–æ–ª–ª–∏—Ä—É–µ–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã"""
        print("   üìú –°–ö–†–û–õ–õ–ò–ú –í–°–ï –ö–û–ù–¢–ï–ô–ù–ï–†–´...")

        try:
            await self._scroll_main_containers(tab)
            await self._scroll_browser_window(tab)
            await self._scroll_all_scrollable_containers(tab)
            await asyncio.sleep(random.uniform(2, 3))

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞: {str(e)[:100]}")

    async def _scroll_main_containers(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã"""
        for selector in self.SELECTORS["scroll_containers"]:
            await tab.evaluate(f"""
                (function() {{
                    const container = document.querySelector('{selector}');
                    if (container && container.scrollHeight > container.clientHeight) {{
                        container.scrollTop = container.scrollHeight;
                        return true;
                    }}
                    return false;
                }})()
            """)
        await asyncio.sleep(random.uniform(1, 2))

    async def _scroll_browser_window(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –æ–∫–Ω–æ –±—Ä–∞—É–∑–µ—Ä–∞"""
        await tab.evaluate("""
            window.scrollBy({
                top: 800,
                behavior: 'smooth'
            });
        """)
        await asyncio.sleep(random.uniform(1, 2))

    async def _scroll_all_scrollable_containers(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –≤—Å–µ —Å–∫—Ä–æ–ª–ª–∏—Ä—É–µ–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã"""
        container_count = await tab.evaluate("""
            document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]').length
        """)

        for i in range(container_count):
            await tab.evaluate(f"""
                (function() {{
                    const containers = document.querySelectorAll(
                        '[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]'
                    );
                    if (containers[{i}]) {{
                        const container = containers[{i}];
                        if (container.scrollHeight > container.clientHeight) {{
                            container.scrollTop = container.scrollHeight;
                        }}
                    }}
                }})()
            """)
            await asyncio.sleep(0.3)

    async def _try_find_pagination_after_scroll(self, tab, current_page: int = 1):
        """–ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏"""
        try:
            html = await tab.get_content()
            soup = BeautifulSoup(html, 'lxml')

            next_page_num = current_page + 1
            all_page_links = self._extract_page_links(soup)

            for href in all_page_links:
                page_num = self._extract_page_number(href)
                if page_num == next_page_num:
                    page_url = self._build_full_url(href)
                    print(f"   üñ± –ù–∞—à–ª–∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}")
                    print(f"   üìç –ü–µ—Ä–µ—Ö–æ–¥–∏–º: {page_url}")

                    await tab.get(page_url)
                    await asyncio.sleep(random.uniform(4, 6))

                    await self._scroll_page_to_bottom(tab)

                    urls_page = await self._get_urls_from_current_page(tab)
                    if urls_page:
                        before = len(self.all_urls)
                        self.all_urls.update(urls_page)
                        new_count = len(self.all_urls) - before
                        print(f"   üìä +{new_count} –Ω–æ–≤—ã—Ö URL")

                    await self._try_find_pagination_after_scroll(tab, next_page_num)
                    return

            print(f"   ‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {next_page_num}")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {str(e)[:60]}")

    def _extract_page_links(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ HTML"""
        page_links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if '/page/' in href or 'page=' in href:
                page_links.append(href)
        return page_links

    def _extract_page_number(self, href: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —Å—Å—ã–ª–∫–∏"""
        page_match = re.search(r'/page/(\d+)', href) or re.search(r'page=(\d+)', href)
        return int(page_match.group(1)) if page_match else None

    def _build_full_url(self, href: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—ã–π URL –∏–∑ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏"""
        if href.startswith('/'):
            return f"https://2gis.ru{href}"
        elif href.startswith('//'):
            return f"https:{href}"
        elif not href.startswith('http'):
            return f"https://2gis.ru{href}"
        return href

    # ========== –û–ë–†–ê–ë–û–¢–ö–ê URL ==========

    async def _get_urls_from_current_page(self, tab) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –º–∞–≥–∞–∑–∏–Ω–æ–≤ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            await asyncio.sleep(1)
            html = await tab.get_content()
            urls = self._extract_urls_from_html(html)
            return self._filter_valid_urls(urls)

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ URL: {str(e)[:50]}")
            return set()

    def _extract_urls_from_html(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –º–∞–≥–∞–∑–∏–Ω–æ–≤ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞"""
        soup = BeautifulSoup(html, 'lxml')
        urls = []

        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if '/firm/' in href:
                full_url = self._normalize_href(href)
                clean_url = self._clean_url(full_url)
                if clean_url and clean_url not in urls:
                    urls.append(clean_url)

        return list(set(urls))

    def _normalize_href(self, href: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –ø–æ–ª–Ω—ã–µ URL"""
        if href.startswith('//'):
            return f"https:{href}"
        elif href.startswith('/'):
            return f"https://2gis.ru{href}"
        elif href.startswith('http'):
            return href
        return ""

    def _filter_valid_urls(self, urls: List[str]) -> Set[str]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö URL –º–∞–≥–∞–∑–∏–Ω–æ–≤"""
        filtered_urls = set()
        for url in urls:
            if self._is_valid_store_url(url):
                clean_url = self._clean_url(url)
                if clean_url:
                    filtered_urls.add(clean_url)
        return filtered_urls

    def _is_valid_store_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –º–∞–≥–∞–∑–∏–Ω–∞"""
        if '/firm/' not in url:
            return False

        exclude_patterns = [
            '/reviews', '/gallery', '/photos', '/menu', '/contacts',
            '/search/', 'tab=', '#', 'reviewTab', 'photoTab'
        ]

        return not any(pattern in url for pattern in exclude_patterns)

    def _clean_url(self, url: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ URL –º–∞–≥–∞–∑–∏–Ω–∞"""
        url = url.split('?')[0].split('#')[0].rstrip('/')
        return self._normalize_href(url)

    # ========== –ü–ê–†–°–ò–ù–ì –°–¢–†–ê–ù–ò–¶ –ú–ê–ì–ê–ó–ò–ù–û–í ==========

    async def _parse_store_pages(self, urls_list: List[str]):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –º–∞–≥–∞–∑–∏–Ω–æ–≤"""
        print(f"\nüè™ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(urls_list)} –º–∞–≥–∞–∑–∏–Ω–æ–≤...")

        for i, url in enumerate(urls_list, 1):
            await self._parse_single_store(i, url, len(urls_list))

    async def _parse_single_store(self, index: int, url: str, total: int):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –º–∞–≥–∞–∑–∏–Ω–∞"""
        print(f"\n   {index}/{total}: {url[:80]}...")

        store_id = self._extract_store_id(url)
        print(f"   ID: {store_id}")

        data = await self._parse_store_page(url)
        if data:
            data['ID'] = store_id

            if self._is_rostov_store(data):
                self.results.append(data)
                self._print_store_info(data)
                print(f"      ‚úÖ –°–û–•–†–ê–ù–Ø–ï–ú")
            else:
                print(f"      üö´ –ü–†–û–ü–£–°–ö–ê–ï–ú (–Ω–µ –∏–∑ –†–æ—Å—Ç–æ–≤–∞-–Ω–∞-–î–æ–Ω—É)")
        else:
            print(f"      ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

        if index < total:
            await asyncio.sleep(random.uniform(2, 4))

    def _extract_store_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –º–∞–≥–∞–∑–∏–Ω–∞ –∏–∑ URL"""
        match = re.search(r'/firm/(\d+)', url)
        return f"2gis_{match.group(1)}" if match else ""

    async def _parse_store_page(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∞–≥–∞–∑–∏–Ω–∞"""
        try:
            print(f"      üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–∞–≥–∞–∑–∏–Ω–∞...")
            page = await self.browser.get(url)
            await asyncio.sleep(random.uniform(3, 5))
            html = await page.get_content()
            soup = BeautifulSoup(html, 'html.parser')
            return self._extract_store_data(url, soup, html)
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None

    def _extract_store_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–∞"""
        data = {
            '–°—Å—ã–ª–∫–∞': url,
            '–ì–æ—Ä–æ–¥': '–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É',
            '–î–∞—Ç–∞ —Å–±–æ—Ä–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            '–ò—Å—Ç–æ—á–Ω–∏–∫': '2GIS'
        }

        data.update(self._extract_store_details(soup))
        return data

    def _extract_store_details(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –º–∞–≥–∞–∑–∏–Ω–∞"""
        details = {
            '–ê–¥—Ä–µ—Å': self._find_address(soup),
            '–¢–µ–ª–µ—Ñ–æ–Ω': self._find_phone(soup),
            '–°–∞–π—Ç': self._find_website(soup),
            '–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã': self._find_opening_hours(soup),
            '–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞': self._find_store_name(soup)
        }

        return details

    def _find_address(self, soup: BeautifulSoup) -> str:
        """–ü–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞ –º–∞–≥–∞–∑–∏–Ω–∞"""
        address = self._find_text_near_svg(soup, self.SVG_PATHS["address"])
        return address or self._find_address_alternative(soup)

    def _find_phone(self, soup: BeautifulSoup) -> str:
        """–ü–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –º–∞–≥–∞–∑–∏–Ω–∞"""
        phones = self._find_phones_near_svg(soup, self.SVG_PATHS["phone"])
        return ', '.join(phones) if phones else self._find_phones_alternative(soup)

    def _find_website(self, soup: BeautifulSoup) -> str:
        """–ü–æ–∏—Å–∫ —Å–∞–π—Ç–∞ –º–∞–≥–∞–∑–∏–Ω–∞"""
        websites = self._find_websites_by_svg(soup, self.SVG_PATHS["website"])
        websites = self._clean_phone_from_website_list(websites)
        return ', '.join(websites[:2]) if websites else ""

    def _find_store_name(self, soup: BeautifulSoup) -> str:
        """–ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –º–∞–≥–∞–∑–∏–Ω–∞"""
        return self._find_store_name_smart(soup)

    def _find_opening_hours(self, soup: BeautifulSoup) -> str:
        """–ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–±–æ—Ç—ã"""
        return self._find_opening_hours_smart(soup)

    # ========== –ú–ï–¢–û–î–´ –ü–û–ò–°–ö–ê –ü–û SVG ==========

    def _find_text_near_svg(self, soup: BeautifulSoup, svg_path: str) -> str:
        """–ù–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º —Å SVG-–∏–∫–æ–Ω–∫–æ–π –ø–æ –ø—É—Ç–∏ d"""
        try:
            svg_path_normalized = re.sub(r'\s+', '', svg_path)

            for path in soup.find_all('path'):
                if path.get('d'):
                    path_d_normalized = re.sub(r'\s+', '', path.get('d'))
                    if self._svg_paths_match(path_d_normalized, svg_path_normalized):
                        address_text = self._search_address_in_svg_container(path)
                        if address_text:
                            return self._clean_address(address_text)

        except Exception as e:
            print(f"      ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ SVG: {e}")

        return ""

    def _svg_paths_match(self, path1: str, path2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ SVG –ø—É—Ç–µ–π"""
        return (path1[:50] == path2[:50] or path1[:100] == path2[:100])

    def _search_address_in_svg_container(self, path_element) -> str:
        """–ü–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ SVG"""
        parent = path_element
        for _ in range(4):
            parent = parent.parent
            if parent:
                address_text = self._extract_address_from_container(parent)
                if address_text:
                    return address_text
        return ""

    def _extract_address_from_container(self, container) -> str:
        """–ò–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
        address_parts = []

        for elem in container.find_all(['div', 'span', 'p']):
            text = elem.get_text(' ', strip=True)
            if text and len(text) > 3 and self._looks_like_address(text):
                address_parts.append(text)

        if address_parts:
            full_address = ', '.join(list(dict.fromkeys(address_parts)))
            if self._is_valid_address(full_address):
                return full_address

        container_text = container.get_text('\n', strip=True)
        return self._extract_address_from_text_lines(container_text)

    def _looks_like_address(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–¥—Ä–µ—Å"""
        address_markers = ['—É–ª.', '—É–ª–∏—Ü–∞', '–ø—Ä.', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–¥.', '–¥–æ–º', '—Ä–æ—Å—Ç–æ–≤']
        return (any(marker in text.lower() for marker in address_markers) or
                text[0].isupper())

    def _is_valid_address(self, address: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∞–¥—Ä–µ—Å–∞"""
        return len(address) > 15 and any(char.isdigit() for char in address)

    def _extract_address_from_text_lines(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ —Å—Ç—Ä–æ–∫ —Ç–µ–∫—Å—Ç–∞"""
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        address_lines = []

        for line in lines:
            line_lower = line.lower()
            if (any(marker in line_lower for marker in ['—É–ª.', '—É–ª–∏—Ü–∞', '–ø—Ä.', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–¥.', '–¥–æ–º', '—Ä–æ—Å—Ç–æ–≤']) and
                    len(line) > 10 and not line.startswith('¬©')):
                address_lines.append(line)

        return ' '.join(address_lines) if address_lines else ""

    def _find_phones_near_svg(self, soup: BeautifulSoup, svg_path: str) -> List[str]:
        """–ù–∞–π—Ç–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã —Ä—è–¥–æ–º —Å SVG-–∏–∫–æ–Ω–∫–æ–π —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        phones = []
        try:
            svg_path_normalized = re.sub(r'\s+', '', svg_path)

            for path in soup.find_all('path'):
                if path.get('d'):
                    path_d_normalized = re.sub(r'\s+', '', path.get('d'))
                    if path_d_normalized[:50] == svg_path_normalized[:50]:
                        phones = self._extract_phones_from_svg_container(path)
                        break

        except Exception as e:
            print(f"      ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

        return phones

    def _extract_phones_from_svg_container(self, path_element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ SVG"""
        phones = []
        parent = path_element

        for _ in range(5):
            parent = parent.parent
            if parent:
                phones.extend(self._extract_tel_links(parent))
                phones.extend(self._extract_phone_text(parent))

        return list(set(phones))

    def _extract_tel_links(self, element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        phones = []
        tel_links = element.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            phone = link.get('href', '').replace('tel:', '').strip()
            if phone:
                phones.append(phone)
        return phones

    def _extract_phone_text(self, element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        phones = []
        text = element.get_text(' ', strip=True)

        for pattern in self.PATTERNS["phone"]:
            for match in re.finditer(pattern, text):
                phone = match.group(0)
                phone_clean = re.sub(r'[^\d\+]', '', phone)
                if len(phone_clean) >= 10:
                    phones.append(phone_clean)

        return phones

    def _find_websites_by_svg(self, soup: BeautifulSoup, svg_path: str) -> List[str]:
        """–ù–∞–π—Ç–∏ —Å–∞–π—Ç—ã —Ä—è–¥–æ–º —Å SVG-–∏–∫–æ–Ω–∫–æ–π"""
        websites = []
        try:
            svg_path_normalized = re.sub(r'\s+', '', svg_path)

            for path in soup.find_all('path'):
                if path.get('d'):
                    path_d_normalized = re.sub(r'\s+', '', path.get('d'))
                    if path_d_normalized[:50] == svg_path_normalized[:50]:
                        websites = self._extract_websites_from_svg_container(path)
                        break

        except Exception as e:
            print(f"      ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–∞–π—Ç–æ–≤: {e}")

        return websites

    def _extract_websites_from_svg_container(self, path_element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–π—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ SVG"""
        websites = []
        parent = path_element

        for _ in range(5):
            parent = parent.parent
            if parent:
                websites.extend(self._extract_websites_from_containers(parent))
                websites.extend(self._extract_websites_from_links(parent))

        return list(set(websites))

    def _extract_websites_from_containers(self, element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–π—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        websites = []
        all_text_containers = element.find_all(['div', 'span', 'a', 'p'])

        for container in all_text_containers:
            container_text = container.get_text(' ', strip=True)
            text_elements = re.split(r'[\s,;]+', container_text)

            for element_text in text_elements:
                element_text = element_text.strip()
                if self._is_website_element(element_text):
                    clean_url = self._normalize_website_text(element_text)
                    if clean_url:
                        websites.append(clean_url)

        return websites

    def _extract_websites_from_links(self, element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–π—Ç–æ–≤ –∏–∑ —Å—Å—ã–ª–æ–∫"""
        websites = []

        for link in element.find_all('a'):
            link_text = link.get_text(' ', strip=True)
            href = link.get('href', '')

            if href.startswith('tel:'):
                continue

            if self._is_phone_text(link_text):
                continue

            if self._is_website_element(link_text):
                clean_url = self._normalize_website_text(link_text)
                if clean_url:
                    websites.append(clean_url)

        return websites

    # ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ==========

    def _clean_phone_from_website_list(self, websites: List[str]) -> List[str]:
        """–û—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Å–∞–π—Ç–æ–≤ –æ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤"""
        return [url for url in websites if not self._is_phone_text(url.replace('https://', '').replace('http://', ''))]

    def _is_phone_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º"""
        if not text:
            return False

        cleaned = re.sub(r'[^\d\+]', '', text)

        phone_patterns = [
            r'^\+7\d{10}$',
            r'^8\d{10}$',
            r'^7\d{10}$',
            r'^\d{10,11}$',
        ]

        for pattern in phone_patterns:
            if re.match(pattern, cleaned):
                return True

        return text.replace(' ', '').replace('-', '').startswith(('+7', '8(', '(8', '7('))

    def _is_website_element(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å–∞–π—Ç–æ–º/–¥–æ–º–µ–Ω–æ–º"""
        if not text or len(text) < 4 or self._is_phone_text(text):
            return False

        for pattern in self.PATTERNS["domain"]:
            if re.match(pattern, text, re.IGNORECASE):
                return True

        if '.' in text:
            parts = text.split('.')
            if len(parts) >= 2:
                last_part = parts[-1].lower()
                common_tlds = ['ru', 'com', 'net', 'org', '—Ä—Ñ', 'su', 'io', 'info', 'biz']
                if last_part in common_tlds:
                    return True

        return False

    def _normalize_website_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ URL"""
        if not text or self._is_phone_text(text):
            return ""

        text = re.sub(r'[\s‚Äí‚Äì‚Äî]', '', text)

        if text.startswith(('+7', '8(', '7(', '(8', '(7')):
            return ""

        if '.' not in text:
            return ""

        if not text.startswith(('http://', 'https://')):
            if text.startswith('www.'):
                text = 'https://' + text
            else:
                text = 'https://' + text

        if 'redirect.2gis' in text.lower() or '2gis.ru' in text.lower():
            return ""

        text = text.split('?')[0].split('#')[0].rstrip('/')

        try:
            result = urlparse(text)
            if result.scheme and result.netloc:
                return text
        except:
            pass

        return ""

    def _find_address_alternative(self, soup: BeautifulSoup) -> str:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞"""
        all_text = soup.get_text(' ', strip=True)

        for pattern in self.PATTERNS["address"]:
            match = re.search(pattern, all_text, re.IGNORECASE)
            if match:
                return match.group(0).strip()

        return ""

    def _clean_address(self, address: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∞–¥—Ä–µ—Å–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Ñ—Ä–∞–∑"""
        if not address:
            return ""

        phrases_to_remove = [
            "–ø–æ–∫–∞–∑–∞—Ç—å –≤—Ö–æ–¥", "–ø–æ–∫–∞–∑–∞—Ç—å –Ω–∞ –∫–∞—Ä—Ç–µ", "–ø–æ–∫–∞–∑–∞—Ç—å —Å—Ö–µ–º—É –ø—Ä–æ–µ–∑–¥–∞",
            "–ø–æ–∫–∞–∑–∞—Ç—å –º–∞—Ä—à—Ä—É—Ç", "–ø–æ–∫–∞–∑–∞—Ç—å –∑–¥–∞–Ω–∏–µ", "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å"
        ]

        cleaned_address = address

        for phrase in phrases_to_remove:
            cleaned_address = cleaned_address.replace(phrase, "")

        if "–ø–æ–∫–∞–∑–∞—Ç—å" in cleaned_address.lower():
            index = cleaned_address.lower().find("–ø–æ–∫–∞–∑–∞—Ç—å")
            if index > 10:
                cleaned_address = cleaned_address[:index]

        cleaned_address = cleaned_address.strip().rstrip(',.; ')
        cleaned_address = re.sub(r'\s+', ' ', cleaned_address)
        cleaned_address = re.sub(r',\s*,', ',', cleaned_address)

        if cleaned_address.endswith(','):
            cleaned_address = cleaned_address[:-1].strip()

        return cleaned_address

    def _find_phones_alternative(self, soup: BeautifulSoup) -> str:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤"""
        phones = []
        all_text = soup.get_text(' ', strip=True)

        for pattern in self.PATTERNS["phone"]:
            for match in re.finditer(pattern, all_text):
                phone = match.group(0)
                phone_clean = re.sub(r'[^\d\+]', '', phone)
                if len(phone_clean) >= 10 and phone_clean not in phones:
                    phones.append(phone_clean)

        return ', '.join(phones) if phones else ""

    def _find_store_name_smart(self, soup: BeautifulSoup) -> str:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –º–∞–≥–∞–∑–∏–Ω–∞"""
        for tag in ['h1', 'h2', 'h3']:
            for elem in soup.find_all(tag):
                text = elem.get_text(' ', strip=True)
                if text and 5 < len(text) < 100:
                    exclude_words = ['–ø–µ—Ä–µ–π—Ç–∏', '–Ω–∞–∑–∞–¥', '–º–µ–Ω—é', '—Ñ–∏–ª—å—Ç—Ä', '–ø–æ–∏—Å–∫']
                    if not any(word in text.lower() for word in exclude_words):
                        return text[:150]

        for meta in soup.find_all('meta'):
            if meta.get('property') in ['og:title', 'og:site_name'] and meta.get('content'):
                content = meta.get('content')
                if content and 5 < len(content) < 150:
                    return content

        all_text = soup.get_text('\n', strip=True)
        lines = [line.strip() for line in all_text.split('\n') if line.strip()]

        for line in lines:
            if (len(line) > 10 and len(line) < 100 and
                    line[0].isupper() and
                    not any(word in line.lower() for word in ['–ø–æ–∏—Å–∫', '–∫–∞—Ç–∞–ª–æ–≥', '—Ñ–∏–ª—å—Ç—Ä'])):
                return line[:150]

        return ""

    def _find_opening_hours_smart(self, soup: BeautifulSoup) -> str:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–±–æ—Ç—ã"""
        all_text = soup.get_text('\n', strip=True)
        lines = all_text.split('\n')

        keywords = [
            '—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã', '–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '—Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã',
            '–æ—Ç–∫—Ä—ã—Ç–æ', '–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã', '—Ä–∞–±–æ—Ç–∞–µ–º', '–ø–Ω-–ø—Ç', '–ø–Ω‚Äì–ø—Ç',
            '–µ–∂–µ–¥–Ω–µ–≤–Ω–æ', '–∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ'
        ]

        for i, line in enumerate(lines):
            line_lower = line.lower()

            if any(keyword in line_lower for keyword in keywords):
                result_lines = [line.strip()]

                for j in range(1, 3):
                    if i + j < len(lines):
                        next_line = lines[i + j].strip()
                        if len(next_line) < 50 and not next_line.startswith('¬©'):
                            result_lines.append(next_line)

                return ' '.join(result_lines)[:150]

        return ""

    def _is_rostov_store(self, data: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –º–∞–≥–∞–∑–∏–Ω –∏–∑ –†–æ—Å—Ç–æ–≤–∞-–Ω–∞-–î–æ–Ω—É"""
        address = data.get('–ê–¥—Ä–µ—Å', '').lower()
        if not address:
            return False

        rostov_patterns = [
            '—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É',
            '—Ä–æ—Å—Ç–æ–≤ –Ω–∞ –¥–æ–Ω—É',
            '–≥.—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É',
            '–≥. —Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É',
            '–≥.—Ä–æ—Å—Ç–æ–≤',
            '–≥. —Ä–æ—Å—Ç–æ–≤'
        ]

        return any(pattern in address for pattern in rostov_patterns)

    # ========== –í–´–í–û–î –ò–ù–§–û–†–ú–ê–¶–ò–ò ==========

    def _print_store_info(self, data: Dict[str, Any]):
        """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–≥–∞–∑–∏–Ω–µ"""
        print(f"      üìç –ê–¥—Ä–µ—Å: {data.get('–ê–¥—Ä–µ—Å', '-')}")
        print(f"      üìû –¢–µ–ª–µ—Ñ–æ–Ω: {data.get('–¢–µ–ª–µ—Ñ–æ–Ω', '-')}")
        print(f"      üåê –°–∞–π—Ç: {data.get('–°–∞–π—Ç', '-')}")
        print(f"      ‚è∞ –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {data.get('–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '-')}")

    def _remove_duplicates(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if not self.results:
            return

        unique_results = []
        seen = set()

        for item in self.results:
            unique_id = self._generate_unique_id(item)

            if unique_id and unique_id not in seen:
                seen.add(unique_id)
                unique_results.append(item)
            else:
                fallback_key = self._generate_fallback_key(item)
                if fallback_key and fallback_key not in seen:
                    seen.add(fallback_key)
                    unique_results.append(item)
                else:
                    unique_results.append(item)

        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nüóë –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed}")
        self.results = unique_results

    def _generate_unique_id(self, item: Dict[str, Any]) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞"""
        url = item.get('–°—Å—ã–ª–∫–∞', '').lower().strip()
        if url:
            match = re.search(r'/firm/(\d+)', url)
            if match:
                return f"2gis_{match.group(1)}"
            return url
        return None

    def _generate_fallback_key(self, item: Dict[str, Any]) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"""
        name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞', '').lower().strip()
        address = item.get('–ê–¥—Ä–µ—Å', '').lower().strip()

        if name and address:
            return f"{name}|{address}"
        elif name:
            return name
        elif address:
            return address
        return None

    def _print_header(self):
        """–í—ã–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        print("=" * 80)
        print("üî• –ü–ê–†–°–ï–† 2GIS: –ú–ê–ì–ê–ó–ò–ù–´ –ü–ò–†–û–¢–ï–•–ù–ò–ö–ò - –†–û–°–¢–û–í-–ù–ê-–î–û–ù–£")
        print("=" * 80)

    def _print_final_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–±–æ—Ä–∞"""
        print("\n" + "=" * 80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê 2–ì–ò–° (–ü–ò–†–û–¢–ï–•–ù–ò–ö–ê)")
        print("=" * 80)

        elapsed_time = time.time() - self.start_time
        minutes = int(elapsed_time // 60)
        seconds = int(elapsed_time % 60)

        print(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"üîó –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(self.all_urls)}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        phones_count = sum(1 for r in self.results if r.get('–¢–µ–ª–µ—Ñ–æ–Ω'))
        sites_count = sum(1 for r in self.results if r.get('–°–∞–π—Ç'))
        hours_count = sum(1 for r in self.results if r.get('–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'))

        print(f"üìû –ú–∞–≥–∞–∑–∏–Ω–æ–≤ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º: {phones_count}")
        print(f"üåê –ú–∞–≥–∞–∑–∏–Ω–æ–≤ —Å —Å–∞–π—Ç–æ–º: {sites_count}")
        print(f"‚è∞ –ú–∞–≥–∞–∑–∏–Ω–æ–≤ —Å –≥—Ä–∞—Ñ–∏–∫–æ–º —Ä–∞–±–æ—Ç—ã: {hours_count}")

        print("\n" + "=" * 80)
